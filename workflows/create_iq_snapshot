#!/usr/bin/env python3

from datetime import datetime
from iqss_gh_reporting import legacy as pdio
from iqss_gh_reporting import pdata as ghpdata
from iqss_gh_reporting import transformer as xfmr
from iqss_gh_reporting import utils as utils
import pandas as pd

import argparse
import os
import re
import yaml
from pathvalidate import sanitize_filename
from pathvalidate import sanitize_filepath
from iqss_gh_reporting import process_labels_util as plu


def main():
    print(f"Running {__file__} as the main program")
    yaml_file = os.path.expanduser(os.getcwd() + '/' + 'input_file.yaml')
    data = utils.read_yaml(yaml_file)
    if data is None:
        raise ValueError(f"Error: there must be a valid input.yaml file in the current working dir: {os.getcwd()}")

    with open(yaml_file) as file:
        ydata = yaml.load(file, Loader=yaml.FullLoader)
    yaml_string = yaml.dump(ydata, default_flow_style=False)
    print(f"input arguments:\n\n{yaml_string}")

    # set workflow specific variables
    summary_of_this_run = '''
        This workflow is used to summarize the size of the cards in a sprint.
        The input is a file that was retrieved from the API. 
        output file names are all synced so that it is obvious that they ran together.
        - the original input 
        - the original  input modified with a size column
        - A one line (2 including header) summary.
        - 4/28/23 23:40 fixed a bug which was including Sprint Ready in ActiveSprint
        '''

    # identify where we are sourcing the data from
    # This is meant to be hardcoded here

    sprint_data = ghpdata.GHProjectData(
        src_type=ydata['src_type']['v'],
        workflow_name=ydata['workflow_name']['v'],
        sprint_name=ydata['sprint_name']['v'],
        collection_flag=ydata['collection_flag']['v'],
        src_dir_name=ydata['src_dir_name']['v'],
        src_file_name=ydata['src_file_name']['v'],
        dest_dir_name=ydata['output_base_dir']['v'],
        organization_name=ydata['organization_name']['v'],
        project_name=ydata['project_name']['v'],
        output_file_base_name=ydata['output_file_base_name']['v'],
        data_collected_time=ydata['data_collected_time']['v']
        )

    if ydata['src_type']['v'] == "file":
        sprint_data.df = utils.read_dataframe_file(
            in_dir=ydata['src_dir_name']['v'],
            file_name=ydata['src_file_name']['v']
            )
        sprint_data.add_log_entry(context="utils", comment=summary_of_this_run)
    elif ydata['src_type']['v'] == "api":
        # get OAUTH token
        auth_token_val = os.getenv('GITHUB_TOKEN', "novalue")
        if auth_token_val == "novalue":
            print("You must set the GITHUB_TOKEN environment variable to run with 'api' flag for this program")
            exit(1)

        sprint_data.df = pdio.LegacyProjectCards(
            access_token=auth_token_val,
            organization_name=sprint_data.organization_name,
            project_name=sprint_data.project_name).df
    else:
        raise ValueError("src_type must be 'file' or 'api'")

    sprint_data.add_log_entry(context="pdio", comment=summary_of_this_run)
    sprint_data.write_log()
    sprint_data.write(postfix="orig")

    xfmrd_df = xfmr.SprintCardSizer(sprint_data).df
    sprint_data.df = xfmrd_df
    sprint_data.write(postfix="sized")

    # This was part of a temporary solution to gluing Ceilyn's code to mine with a bash script
    # with open("runlog.sh", 'a') as f:
    #     f.write(f"DEST_DIR_NAME={sprint_data.dest_dir_name}\n")
    #     f.write(f"DEST_FILE_SIZED={sprint_data.dest_file_name}-sized.tsv\n")

    # prs = transformer.PrPointsFetcher(sprint_data)
    # utils.write_dataframe(df=prs.df_zero_rows())

    # SprintSizeSummarizer creates a new dataframe so I need create a variable that I can reference to get the results
    summarized_data = xfmr.SprintSizeSummarizer(sprint_data)
    summarized_data.write(postfix="summary")

    # create a matrix of the data
    # communicate what is needed using a Yaml file
    # Todo: this is a temporary Solution for the V1.0 MVP
    # epilog='Usage: python process_labels.py --input <infile> --output <ofile> --matrix <matrixfile>')

    # read the input file
    snapshot_file = sprint_data.dest_dir_name + "/" + sprint_data.dest_file_name + "-sized.tsv"
    df = plu.read_sized_snapshot(snapshot_file)

    # process the labels in the file
    new_df = plu.process_sized_snapshot(df, labels='Labels', key='CardURL')

    # write to matrix file
    output_file_name = sprint_data.dest_dir_name + "/" + sprint_data.dest_file_name + "-matrix.tsv"
    print(f"Saving results to file.\n{output_file_name}")
    print(output_file_name)
    new_df.to_csv(output_file_name, sep='\t', index=False)

    # get unique values for column=Column
    values = new_df['Column'].unique()

    # define filter substrings
    substrs = ['This Sprint','In Progress','Ready for Review','In Review']

    # create filter
    filter = plu.create_sprint_filter('Column', values, substrs)

    # summarize a subset of the processed snapshot
    output_df = plu.summarize_processed_sized_snapshot(new_df, filter=filter)

    # 1 write summary to files
    output_file_name = sprint_data.dest_dir_name + "/" + sprint_data.dest_file_name + "-1line_matrix.tsv"
    print(f"Saving results to file.\n{output_file_name}")
    output_df.to_csv(output_file_name, sep='\t', index=False)

    # create size summary
    # exclude some fields when modifying values to create the special matrix of sizes
    excluded = ['CardURL','Project','Column','Card',
                'Type','Number','Labels','Repo','State',
                'CreatedAt','UpdatedAt','ClosedAt','LinkedPRIssues',
                'card','Size']
    # create a matrix dataframe using issue size values
    new_matrix_df = plu.create_label_sized_matrix(new_df, size='Size', exclude=excluded)

    output_file_name = sprint_data.dest_dir_name + "/" + sprint_data.dest_file_name + "-new_matrix_sized.tsv"
    print(f"Saving results to file.\n{output_file_name}")
    new_matrix_df.to_csv(output_file_name,sep='\t')

    # summarize the matrix
    summarized_matrix = plu.summarize_processed_sized_snapshot(new_matrix_df, filter=filter)

    # write summary to file
    output_file_name = sprint_data.dest_dir_name + "/" + sprint_data.dest_file_name + "-1line_matrix_sized.tsv"
    print(f"Saving results to file.\n{output_file_name}")
    summarized_matrix.to_csv(output_file_name,sep='\t')

    # tack the two 1 line summaries together
    final_df = pd.concat([summarized_data.df_summary, summarized_matrix], axis=1)

    # write summary to files
    output_file_name = sprint_data.dest_dir_name + "/" + sprint_data.dest_file_name + "-1line_long_matrix.tsv"
    print(f"Saving results to file.\n{output_file_name}")
    final_df.to_csv(output_file_name, sep='\t', index=False)



if __name__ == "__main__":
    main()
