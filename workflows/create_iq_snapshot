#!/usr/bin/env python3

from datetime import datetime
from iqss_gh_reporting import legacy as pdio
from iqss_gh_reporting import pdata as ghpdata
from iqss_gh_reporting import transformer as xfmr
from iqss_gh_reporting import utils as utils
import argparse
import os
import re
import yaml
from pathvalidate import sanitize_filename
from pathvalidate import sanitize_filepath


def main():
    print(f"Running {__file__} as the main program")
    yaml_file = os.path.expanduser(os.getcwd() + '/' + 'input_file.yaml')
    data = utils.read_yaml(yaml_file)
    if data is None:
        raise ValueError(f"Error: there must be a valid input.yaml file in the current working dir: {os.getcwd()}")

    with open(yaml_file) as file:
        ydata = yaml.load(file, Loader=yaml.FullLoader)
    yaml_string = yaml.dump(ydata, default_flow_style=False)
    print(f"input arguments:\n\n{yaml_string}")

    # set workflow specific variables
    summary_of_this_run = '''
        This workflow is used to summarize the size of the cards in a sprint.
        The input is a file that was retrieved from the API. 
        output file names are all synced so that it is obvious that they ran together.
        - the original input 
        - the original  input modified with a size column
        - A one line (2 including header) summary.
        - 4/28/23 23:40 fixed a bug which was including Sprint Ready in ActiveSprint
        '''

    # identify where we are sourcing the data from
    # This is meant to be hardcoded here

    sprint_data = ghpdata.GHProjectData(
        src_type=ydata['src_type'],
        workflow_name=ydata['workflow_name'],
        sprint_name=ydata['sprint_name'],
        collection_flag=ydata['collection_flag'],
        src_dir_name=ydata['src_dir_name'],
        src_file_name=ydata['src_file_name'],
        dest_dir_name=sanitize_filepath(ydata['output_base_dir'] + '/' + data['sprint_name'], platform="auto"),
        data_collected_time=ydata['collection_timestamp'],
        organization_name=ydata['organization_name'],
        project_name=ydata['project_name']
        )

    if ydata['src_type'] == "file":
        sprint_data.df = utils.read_dataframe_file(
            in_dir=ydata['src_dir_name'],
            file_name=ydata['src_file_name']
            )
        sprint_data.add_log_entry(context="utils", comment=summary_of_this_run)
    elif ydata['src_type'] == "api":
        # get OAUTH token
        auth_token_val = os.getenv('GITHUB_TOKEN', "novalue")
        if auth_token_val == "novalue":
            print("You must set the GITHUB_TOKEN environment variable to run with 'api' flag for this program")
            exit(1)

        sprint_data.df = pdio.LegacyProjectCards(
            access_token=auth_token_val,
            organization_name=sprint_data.organization_name,
            project_name=sprint_data.project_name).df
    else:
        raise ValueError("src_type must be 'file' or 'api'")

    sprint_data.add_log_entry(context="pdio", comment=summary_of_this_run)
    sprint_data.write_log()
    sprint_data.write(postfix="orig")

    xfmrd_df = xfmr.SprintCardSizer(sprint_data).df
    sprint_data.df = xfmrd_df
    sprint_data.write(postfix="sized")
    with open("runlog.sh", 'a') as f:
        f.write(f"DEST_DIR_NAME={sprint_data.dest_dir_name}\n")
        f.write(f"DEST_FILE_SIZED={sprint_data.dest_file_name}-sized.tsv\n")

    # prs = transformer.PrPointsFetcher(sprint_data)
    # utils.write_dataframe(df=prs.df_zero_rows())

    # SprintSizeSummarizer creates a new dataframe so I need create a variable that I can reference to get the results
    summarized_data = xfmr.SprintSizeSummarizer(sprint_data)
    summarized_data.write("summary")

if __name__ == "__main__":
    main()
